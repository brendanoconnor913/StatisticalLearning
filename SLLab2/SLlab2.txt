2.a)
i. Yes there is a positive relationship between horsepower and mpg so as
horsepower increases mpg increases.
ii. The coefficient is estimated at 0.11080 with small p values which indicate there is a decently strong relationship.
iii. positive (coefficient is positive)
iv. 
       fit      lwr      upr
1 28.66647 27.36905 29.96389
fit is the predicted value of mpg with horsepower at 98 with lwr and 
upr indicating the confidence interval.
       fit     lwr      upr
1 28.66647 14.6462 42.68674
the same holds for this table but lwr and upr are for the predicted intervals
c) yes there is clearly a problem: there is a discontinous jump in the data and it appears that a better model would subdivide the 
data and fit two separate lines to it between the different intervals.
This has thrown off our analysis and in some ways invalided the previous
results.

3.
b)
                    mpg  cylinders displacement horsepower     weight
mpg           1.0000000 -0.7762599   -0.8044430  0.4228227 -0.8317389
cylinders    -0.7762599  1.0000000    0.9509199 -0.5466585  0.8970169
displacement -0.8044430  0.9509199    1.0000000 -0.4820705  0.9331044
horsepower    0.4228227 -0.5466585   -0.4820705  1.0000000 -0.4821507
weight       -0.8317389  0.8970169    0.9331044 -0.4821507  1.0000000
acceleration  0.4222974 -0.5040606   -0.5441618  0.2662877 -0.4195023
year          0.5814695 -0.3467172   -0.3698041  0.1274167 -0.3079004
origin        0.5636979 -0.5649716   -0.6106643  0.2973734 -0.5812652
             acceleration       year     origin
mpg             0.4222974  0.5814695  0.5636979
cylinders      -0.5040606 -0.3467172 -0.5649716
displacement   -0.5441618 -0.3698041 -0.6106643
horsepower      0.2662877  0.1274167  0.2973734
weight         -0.4195023 -0.3079004 -0.5812652
acceleration    1.0000000  0.2829009  0.2100836
year            0.2829009  1.0000000  0.1843141
origin          0.2100836  0.1843141  1.0000000

c)

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -2.128e+01  4.259e+00  -4.998 8.78e-07 ***
cylinders    -2.927e-01  3.382e-01  -0.865   0.3874    
displacement  1.603e-02  7.284e-03   2.201   0.0283 *  
horsepower    7.942e-03  6.809e-03   1.166   0.2442    
weight       -6.870e-03  5.799e-04 -11.846  < 2e-16 ***
acceleration  1.539e-01  7.750e-02   1.986   0.0477 *  
year          7.734e-01  4.939e-02  15.661  < 2e-16 ***
origin        1.346e+00  2.691e-01   5.004 8.52e-07 ***

F-statistic: 256.7 on 7 and 389 DF,  p-value: < 2.2e-16

i. Because we have a valid F-statistic we no there is a relationship
between the response and the predictors.
ii. All the predictors are valid (none with invalid p values) but the 
factors with more ideal p values are origin, year, and weight.
iii. The p value in combination with the coefficient for year (.7) 
indicate it is one of the strongest predictors for determining mpg.

d) 

There is clearly a problem with the data as there is a bend in the
plot of residuals indicating correlations among the factors. There are
also a couple of leverage points that are likely distorting the results
of the regression.

e)

Weight and displacement result in a very low p-value as do cylinders
and year. It seems that cylinders and weight interact with multiple variables
and so adding an interaction term helps to explain some of those results.

f) 

In taking different functions on the factors it doesn't seem to have changed
the model much. When applied on variables that were already significant it 
produced significant results but it wasn't able to make insignificant 
variables matter if anything it decreased their predictive capabilities.

4.
b) 
For the categorical variables (Urban and US) we cannot use the magnitude
of the coefficients to say anything meaningful because fitting a line to
categorical variables doesn't work well. However we can tell from the sign
of the coefficient whether it has a positive or negative impact on sales.
Since the urban value has a high p value we can say that it is not valid. 
However, for the US the coefficient is positive and has a valid p value
so we know it has a positive impact on sales. Price is a quantitative 
variable and has a valid p value so we know that its coefficient is valid.
We see that it has a slight negative effect on sales.

c) 
sales = 13.04-0.0544(Price)+1.2(US)

d) Price and US because they have strong p values.

f) Looking at the values it looks like the difference in the models it 
materially unchanged. Much of the coefficients for the valid variables 
are the same in each model indicating that Urban really had no effect
on the previous model.

g) 
                  2.5 %      97.5 %
(Intercept) 11.79032020 14.27126531
Price       -0.06475984 -0.04419543
USYes        0.69151957  1.70776632

h) There are some outliers and high leverage points but it seems that
for the most part the model it pretty solid and it is mostly linear.

5.
a)
Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
x   1.9939     0.1065   18.73   <2e-16 ***

We see that the p value is valid and the t value large indicating that x
has a strong correlation with y. We see that the estimated coefficient is
approximately 2 which is what we would expect from how we constructed y. 

b)
Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
y  0.39111    0.02089   18.73   <2e-16 ***

The results are in the ball park of expectation (t and p values are the 
exact same) however the estimate of .3911 is low as that corresponds to
1/.399 ~ 2.5 which is higher than the coefficient we calculated at 2.
However the standard error is significantly smaller.

c) 
Obviously a and b are related by the forumla we used to calculate them 
and the coefficients are just the inverses of one another. The interesting
part is that the standard errors differ

d) 
> tstat = function(x, y) {
+     num = (sqrt(length(x)-1)*sum(x*y))
+     denom = (sqrt(sum(x^2)*sum(y^2)-sum(x*y)^2))
+     result = (num/denom)
+     return(result)
+ }
> tstat(x,y)
[1] 18.72593
> tstat(y,x)
[1] 18.72593
note this result is the same as calculated in the summary of each
regression.

e)
As we can see from the results of the tstat function it doesn't matter
if we assume a regression x onto y or y onto x the numerical results 
are the same. 

f)
Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8768 -0.6138 -0.1395  0.5394  2.3462 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.03769    0.09699  -0.389    0.698    
x            1.99894    0.10773  18.556   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9628 on 98 degrees of freedom
Multiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 
F-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16

Call:
lm(formula = x ~ y)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.90848 -0.28101  0.06274  0.24570  0.85736 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.03880    0.04266    0.91    0.365    
y            0.38942    0.02099   18.56   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4249 on 98 degrees of freedom
Multiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 
F-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16

We see that the t values are the same in each but different from 
when we performed the regression with no intercept.
