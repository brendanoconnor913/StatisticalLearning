> y = 1*(.5*x1 - x2 > 0)
> View(y)
> xdf = data.frame(y,x1,x2)
> plot(x2~x1,data=xdf,col=y+2)
> cv.error = double(20)
> test = seq(from=0,to=100,by=5)
> cv.error = double(21)
> svmfit = svm(y~.,data=xdf,scale=FALSE,kernel="linear",cost=5)
> svm.pred = predict(svm,xdf)
Error in UseMethod("predict") : 
  no applicable method for 'predict' applied to an object of class "function"
> svm.pred = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> svmfit = svm(y~.,data=xdf,scale=FALSE,kernel="linear",cost=1)
> svm.pred = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> svmfit = svm(y~.,data=xdf,scale=FALSE,kernel="linear",cost=200)
> svm.pred = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> View(svm.pred)
> svm.prob = predict(svmfit,xdf)
> svm.pred = ifelse(svm.prob > .5, 1, 0)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> svmfit = svm(y~.,data=xdf,scale=FALSE,kernel="linear",cost=5)
> svm.prob = predict(svmfit,xdf)
> svm.pred = ifelse(svm.prob > .5, 1, 0)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> svm.pred = ifelse(svm.prob > 0, 1, 0)
> plot(x2~x1,data=xdf,col=svm.pred+3)
> svmfit = svm(factor(y)~.,data=xdf,scale=FALSE,kernel="linear",cost=5)
> svm.prob = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=svm.prob+3)
Warning message:
In Ops.factor(svm.prob, 3) : ‘+’ not meaningful for factors
> View(svm.prob)
> svmfit = svm(y~.,data=xdf,scale=FALSE,kernel="linear",cost=5)
> svm.prob = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=svm.prob+3)
> svmfit = svm(factor(y)~.,data=xdf,scale=FALSE,kernel="linear",cost=5)
> svm.prob = predict(svmfit,xdf)
> plot(x2~x1,data=xdf,col=as.numeric(svm.prob)+3)
> 
> 
> 
> 
> View(as.numeric(svmfit))
Error in View : (list) object cannot be coerced to type 'double'
> View(svmfit)
Error in View : cannot coerce class "c("svm.formula", "svm")" to a data.frame
> View(svm.pred)
> err = sum(ifelse(svm.prob==y,1,0))
> err = sum(ifelse(svm.prob==y,0,1))/500
> ?tune.svm
> tc = tune.control(cross=5)
> svmfit = tune.svm(cat(x1,x2),y,cost=5,tunecontrol=tc)
-0.0814355 -0.03982127 -0.2716106 0.4201503 -0.4843002 0.07145193 0.4690749 -0.2644619 -0.102804 -0.1209069 0.2990931 0.3425594 -0.4344613 0.09314377 0.4209676 0.1600645 -0.3049957 0.4897363 -0.1072578 0.3509938 -0.1958692 -0.314107 -0.1925409 -0.2236965 -0.3769454 0.4196943 0.2702336 0.06666574 -0.3677711 0.3898967 -0.1388473 -0.2883434 0.229198 0.3252181 -0.04781067 0.07441908 -0.230989 0.4722605 0.1583402 0.4223001 -0.03269509 -0.05941592 0.08892415 0.3557762 -0.09366723 -0.01702139 -0.4511658 -0.2003353 0.124743 0.315534 0.1541821 0.269792 -0.119964 -0.3282397 0.3145658 0.323823 -0.1214291 0.2587697 0.279524 -0.4896958 0.2608373 -0.3719167 0.4126288 -0.3516557 -0.07785614 0.05074615 0.4121575 0.08500913 -0.1339928 0.1869941 -0.214047 -0.3486929 -0.3312692 0.114021 0.2321691 -0.06186386 -0.1589622 -0.1145298 -0.2495839 0.2379052 0.2308746 0.002763636 -0.4918336 -0.1849087 -0.3801151 0.1417866 0.1010905 0.05537442 -0.03564751 -0.1986672 0.2048158 0.4679042 -0.1102816 0.02092 0.08811062 -0.0853115 0.3857608 -0.3912452 0.4192117 0.4167019 -0.1896663 -0.08170494 0.03585787 -0.2320117 0.2911179 0.1838244 0.4964229 -0.03824572 0.1554553 0.01403454 0.4956695 -0.1480244 -0.4444214 0.09204729 0.1253409 0.4861307 0.1797576 0.2012269 -0.3042448 -0.06464156 -0.06894035 -0.007077476 0.2382446 0.2058243 -0.4164709 -0.3239046 -0.1121094 0.04944502 0.02418618 0.2228415 -0.1475587 -0.2424211 0.09610701 0.4029765 0.3084349 0.1813218 0.2883249 -0.4704906 -0.3354517 0.4956585 -0.3968567 -0.1201016 0.2159611 0.4911392 0.1910233 -0.2379458 0.406159 -0.3150783 -0.001015243 -0.1224935 -0.05965196 -0.4085023 -0.1345284 -0.1842162 0.3014432 -0.1531143 0.3379047 0.410938 0.2854246 0.1345011 -0.3271403 -0.08866153 -0.3160327 0.1917015 0.1803791 0.0603687 0.4242343 -0.3586644 -0.3972421 -0.3451175 0.2824886 -0.1107816 -0.3505625 0.00301874 0.1618718 0.1354924 -0.2828488 -0.007389633 -0.08304631 -0.4790233 0.4165416 0.1891386 -0.02077849 0.1585663 0.4513609 -0.184397 0.2854553 0.1353731 -0.2237259 -0.180833 -0.4450164 0.09118747 0.06691193 -0.3389286 0.06642918 -0.3277118 0.01401433 -0.108116 -0.1300407 -0.429405 -0.04138749 -0.2151845 -0.2467093 0.4080909 -0.4335945 -0.0005306483 0.4449577 0.01300682 0.09719276 -0.2978533 -0.3473361 -0.3013708 0.1744186 -0.2252665 0.2106256 -0.1552891 -0.3627694 -0.4145661 0.3883513 -0.4222796 0.4381884 0.4144499 0.006922321 -0.2828795 0.03651503 -0.2768053 -0.08009085 0.04169189 -0.2359717 0.4428052 0.009701199 -0.1327936 0.4352139 -0.01792682 0.2297394 -0.3162325 0.2803329 0.05167324 -0.2389906 -0.1000206 -0.2870081 -0.1549034 -0.1144166 -0.124829 0.462694 0.2254782 0.1760863 -0.019196 -0.0951373 -0.3198859 0.3718808 -0.1394146 -0.1774458 0.2740221 0.2153362 0.4914234 0.05181471 0.125629 -0.001243686 0.2177686 -0.4616541 0.3604858 0.4873527 -0.3985014 -0.1047075 -0.2401529 -0.08288484 -0.3150535 -0.4312051 0.2399931 0.01282233 0.1597691 0.3429669 -0.4998109 0.4206815 -0.4625938 -0.4073137 0.4287682 0.3214066 -0.1472326 -0.4621559 -0.376089 -0.4468906 -0.2734038 -0.1845129 0.3063687 -0.1274348 0.06751892 -0.4225342 -0.2676819 0.04938169 -0.4042092 0.4225332 0.05539967 -0.001071851 0.4101211 -0.1375218 0.07843399 0.02306705 -0.05364073 0.4748352 -0.01324774 0.2380346 -0.4029798 0.05827532 0.3828929 -0.4191483 0.375377 -0.1841663 -0.2454161 0.2410358 -0.4931165 0.1572651 -0.4940191 0.2859335 0.09461784 -0.07594097 -0.3991783 -0.005409161 0.2727058 -0.1789447 -0.01764753 0.1020754 0.1099538 0.1254942 -0.4013376 -0.3273513 -0.2748799 0.1440433 -0.3947572 -0.2516949 -0.007021051 0.4168011 0.05164935 0.04828287 -0.1874482 -0.1996404 -0.3282245 -0.3074343 0.3327955 -0.0311545 -0.4279996 -0.3113657 -0.2091068 0.161693 -0.2966662 -0.2235082 0.4138844 -0.2411559 -0.4382005 0.08679209 0.3818441 -0.1706433 0.1574495 -0.310997 0.0893897 -0.4037111 -0.4774812 0.321056 0.1734026 -0.4496006 -0.005894246 0.187351 -0.3080111 0.418641 0.05874395 0.4453881 0.03084215 0.3002163 -0.2591281 -0.3694596 0.3770917 0.3128643 0.4581361 0.3916922 -0.202237 0.2267581 -0.1027537 -0.06650854 -0.2678026 -0.1564857 0.04687968 0.01394054 -0.3878912 0.4801364 -0.4268484 -0.2352153 -0.4599556 0.2249494 -0.2643173 0.4776219 -0.4261394 -0.1803807 -0.1706792 0.39866 -0.4798714 -0.2946289 0.4908612 0.3058661 -0.1463275 0.4761979 -0.2512866 -0.06897581 -0.3308508 -0.4091838 -0.2783185 -0.4056382 -0.4545023 -0.2681598 0.1449278 -0.1171559 -0.1896018 0.1988405 -0.3725069 0.1503239 0.3867938 -0.2133592 0.4200081 0.4212521 -0.01461095 -0.02110254 0.3454516 -0.1734524 0.1750248 0.2564477 0.007916329 0.339037 -0.3253816 0.1146173 0.08549381 -0.3338935 -0.07730426 0.1969294 0.3341953 -0.07795357 0.06804424 -0.3392386 0.2055007 0.3901248 0.1725385 0.3072833 -0.2754376 -0.3411101 0.3918077 0.2452976 -0.2019231 -0.1077781 -0.3659071 -0.2102096 -0.4520371 0.3340824 0.4001502 0.3870052 0.4952546 -0.1372227 0.4951789 -0.3983124 -0.09412942 0.4095017 0.09906836 0.4226107 -0.3777519 0.3364357 -0.4022516 0.3260859 0.1920734 -0.07947751 -0.0002956649 0.3519513 0.1414716 -0.130634 0.002786872 -0.386674 -0.1905363 0.421448 -0.3311792 0.01937578 -0.1859943 0.1005023 -0.05483996 -0.1086739 0.3904234 0.3957969 -0.3670243 0.1124961 -0.2337816 0.21777 -0.4259025 -0.09676416 -0.2265827 0.2275901 0.1404649 0.2277878 0.4857566 0.3115359 0.4632995 0.1151229 -0.1465947 0.03603432 -0.2305019 0.1280803 -0.395707 0.1764478 0.2032831 -0.09099222 0.4523482 -0.3764157 -0.3115985 0.2269113 0.1602083 -0.3403948 -0.1055413 0.09259268 -0.1977167 0.01138023 0.3660451 0.1908167 -0.2575644 -0.269908 0.009684521 -0.4144588 0.1593807 -0.37208 0.45869 -0.3646824 0.3794659 -0.4700472 -0.2239836 -0.1376387 0.2169882 0.3759261 0.4895886 0.05513221 0.2658739 0.01276719 0.2237768 0.3045382 0.1503119 -0.3164779 0.3919022 0.2780322 -0.01335672 -0.08471333 0.4856452 -0.3708679 0.06143492 -0.1471316 -0.4084067 0.007431524 -0.2336308 -0.1913576 0.2879911 0.3754424 -0.4077396 -0.1251356 -0.008143849 -0.01924138 -0.02002188 -0.1971868 0.1731559 0.3242855 0.2644488 0.4562021 -0.3400739 0.2285732 0.0833557 -0.3084744 0.2784173 0.1050765 -0.2784798 0.1391384 -0.4575679 0.05092513 0.3974831 0.2356508 -0.3603897 -0.4696635 0.4211694 -0.03264204 -0.3776527 0.2653836 -0.3420137 0.3669543 -0.07544596 -0.3574281 -0.4475729 -0.3298395 -0.4832481 0.1294366 0.232879 0.08837086 0.4577105 0.08637539 -0.2499656 -0.01120812 -0.2634446 0.3605264 -0.38695 -0.007048636 0.2274101 0.1083196 -0.4817907 0.2819157 0.1551015 0.2246898 0.4298059 0.4050037 -0.44771 -0.4744219 -0.3455454 -0.2950075 -0.04896581 -0.06108768 0.4800659 0.2273289 0.4399406 0.3801865 -0.1008166 -0.4022943 -0.1107175 0.3339494 -0.1513639 -0.064563 0.005603036 0.18679 -0.3250925 -0.2295559 -0.01133813 -0.2480148 0.1754143 0.1239881 -0.3502217 0.03238241 -0.05743997 -0.07727891 -0.3915543 -0.00455077 -0.1062909 -0.4480656 0.03260954 -0.1312509 -0.3863762 -0.3525325 0.1394524 0.1841639 0.2363543 -0.3954539 0.2576679 -0.3291597 0.4757482 -0.4676116 0.3366764 -0.2413448 0.2568985 0.415501 -0.3662793 -0.3223203 0.4265643 -0.3579408 0.3609346 -0.2764606 0.02175247 0.3144117 -0.4228053 -0.1596267 -0.2729979 -0.4780072 -0.297537 0.4071046 -0.02453309 -0.3695644 -0.2568852 0.1774757 -0.2663324 0.1781479 0.3408032 0.4807218 0.2723879 -0.4039438 0.2115967 -0.1146894 -0.4388765 -0.4644931 -0.383706 -0.1957418 -0.04927391 0.08736966 0.3308603 0.4406512 0.3002617 -0.2430543 -0.07211957 -0.4064312 -0.4965975 -0.0857678 -0.1873604 0.406903 0.007636934 -0.1476658 0.09041639 -0.3943796 0.1796777 0.4509246 0.1332927 -0.1067826 -0.4121673 -0.3272534 0.1632541 0.3519928 0.4848324 -0.2144701 0.3145402 0.3652073 0.09337528 0.1078725 0.1488941 0.3761256 0.446501 -0.4306942 -0.3528717 0.348304 -0.1182572 -0.4513831 0.2406958 0.2703037 -0.2420234 0.07423113 0.2646512 0.1195276 0.1473848 -0.3864962 0.3915847 -0.009479322 0.3997904 0.05417931 -0.1548891 0.08660131 -0.03028892 0.3907739 -0.1694109 0.08208501 0.2147397 0.1011924 -0.1139391 0.300803 -0.2864886 -0.001843431 0.4887515 0.09155422 -0.2951612 -0.2437065 -0.493379 0.3451299 0.464496 0.2397503 0.2481611 0.4212786 0.02338995 -0.1838157 0.4071954 0.02191966 0.3300843 0.04196145 0.0376635 -0.3561369 -0.1546073 0.4594407 -0.0155839 0.2083748 -0.3497719 -0.2791895 0.3544082 -0.143053 -0.3566294 0.4630861 -0.4428934 0.4259329 0.1158503 -0.3327787 0.314433 -0.1535848 -0.492856 0.180665 -0.4190769 -0.0009805546 0.1023134 -0.08230831 -0.06314037 0.341969 -0.02237595 0.3963269 -0.4001433 0.3214088 -0.3212483 0.1547722 -0.4719205 -0.03524344 -0.2122142 0.2321829 0.2711785 0.29285 0.4725154 -0.3081148 -0.2857586 -0.1686146 -0.3204009 -0.03901398 -0.01190686 -0.2444942 0.3248999 -0.2517738 -0.1481408 -0.3151679 -0.3956008 0.4234778 -0.1296049 -0.3390722 -0.05234538 -0.1245625 0.4547083 0.05672335 0.3069633 0.4206111 -0.04639263 -0.3748679 -0.06908902 0.2036224 -0.230796 0.236188 0.161337 0.20376 -0.342894 -0.4010255 -0.4543001 -0.4090676 0.1798298 -0.4967315 0.05114631 0.2417419 -0.3504028 -0.2256744 -0.06326842 0.1317442 -0.1848305 -0.3034178 0.1633088 -0.2237043 -0.4219511 -0.3855347 -0.3136964 0.2916067 -0.3277249 -0.2604884 0.4795903 -0.03659492 -0.08230343 -0.1852435 -0.1978735 -0.374567 -0.1215596 0.4074024 0.1241614 0.0807612 -0.1898507 -0.1746419 -0.4055909 -0.04767439 -0.2767132 -0.2864761 -0.4633874 0.282438 -0.4862552 -0.2524058 0.4690209 -0.398943 0.1531235 -0.02500267 -0.4219119 -0.3818742 -0.1813455 -0.1120098 0.4929381 -0.414509 0.04817431 -0.4851888 0.4554763 0.4362951 0.09286613 -0.03695633 0.001483437 -0.4642126 -0.1198556 0.4883466 -0.2952258 -0.49169 -0.07592428 0.3179438 -0.2895267 0.1773724 -0.1711356 0.3252687 0.06902052 -0.199878 0.07103641 0.3586098 0.1327624 0.2772879 0.3819835 0.2674659 -0.1388062 0.4426789 0.06298078 0.06005245 -0.0406932 0.1085158 0.103676 -0.307236 0.492506 0.09271963 0.4956227 0.1773848 -0.04789112 -0.3180856 0.3896223 -0.04246995 -0.2950971 -0.1750944 -0.287496 -0.01867596 0.1419094 0.1352992 -0.009335526 0.2271064 0.2927874 0.3667729 0.05370192 0.01712562 0.2023421 0.2327219 0.4982088 0.03238419 0.4717619 -0.3686593 -0.07745728 0.3423763 -0.3291375 0.1410734 -0.1002648 -0.2129459 0.3650333 -0.4025226 0.3009881 -0.2240468 0.4451587 0.1632607 0.3968253 0.3207735 -0.2034429 -0.03397221 -0.406312 -0.2590933 0.1884248 0.2189333 0.4410786 -0.3675587 -0.299432 0.08358601 -0.1206501 0.3518842 0.02023697 0.4335646 -0.4652125 0.2665301 0.226045 -0.27189 -0.1250529 0.122206 0.1574255 -0.1420844 0.4203656 0.219078 0.1421877 -0.1566585 0.387094 0.443108 0.3910769 -0.1776086 0.3038999 0.1990731 0.4690411 0.4216944 -0.3544607 -0.2277617 -0.3878759 -0.4376272 0.2241975 -0.181839 -0.416544 0.3692112 0.1789462
Error in if (tunecontrol$cross > n) stop(sQuote("cross"), " must not exceed sampling size!") : 
  argument is of length zero
> ?tune.control
> allerror = double(20)
> for(k=1:5){
Error: unexpected '=' in "for(k="
>     if(k==1){
+         train = 200:500
+     }
Error: object 'k' not found
>     else {
Error: unexpected 'else' in "    else"
>         train = (k-1)*100:(k*100)-1
Error: object 'k' not found
>     }
Error: unexpected '}' in "    }"
> }
Error: unexpected '}' in "}"
> for(k in 1:5){
+     if(k==1){
+         train = 200:500
+     }
+     else {
+         train = (k-1)*100:(k*100)-1
+     }
+ }
> for(k in 1:5){
+     if(k==1){
+         train = 200:500
+     }
+     else {
+         train = 1:500
+         train[(k-1)*100:(k*100)-1] = NULL
+     }
+     
+ }
Error in train[(k - 1) * 100:(k * 100) - 1] <- NULL : 
  replacement has length zero
> ?createFolds
No documentation for ‘createFolds’ in specified packages and libraries:
you could try ‘??createFolds’
> library(caret)
Error in library(caret) : there is no package called ‘caret’
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(mlbench)
> ?createFolds
> testfolds = createFolds(xdf,5)
> testfolds = createFolds(xdf,k=5)
> str(testfolds)
List of 3
 $ Fold1: int 1
 $ Fold2: int 2
 $ Fold3: int 3
> testfolds = createFolds(xdf)
> ?caret
No documentation for ‘caret’ in specified packages and libraries:
you could try ‘??caret’
> testfolds = createFolds(xdf, times=5)
Error in createFolds(xdf, times = 5) : unused argument (times = 5)
> testfolds = createDataPartition(xdf,times=5)
Warning messages:
1: In createDataPartition(xdf, times = 5) :
  Some classes have no records (  ) and these will be ignored
2: In createDataPartition(xdf, times = 5) :
  Some classes have a single record (  ) and these will be selected for the sample
> testfolds = createFolds(xdf,k=5)
> testfolds = createFolds(x1,k=5)
> testfolds$Fold1
  [1]   4   6   7   9  13  21  44  48  49  52  57  69  72
 [14]  74  76  80  84  87  89  90  97  98 107 109 120 125
 [27] 127 143 146 148 162 168 179 191 192 194 197 211 213
 [40] 219 230 233 235 247 249 251 252 254 257 259 269 271
 [53] 274 283 290 292 298 303 310 319 339 354 366 367 372
 [66] 373 396 397 398 403 406 408 410 413 414 421 424 429
 [79] 430 436 441 444 445 446 448 449 450 453 454 458 462
 [92] 465 466 470 471 473 480 483 490 500
> testtest = 1:500
> testtest[-testfolds$Fold1]
  [1]   1   2   3   5   8  10  11  12  14  15  16  17  18
 [14]  19  20  22  23  24  25  26  27  28  29  30  31  32
 [27]  33  34  35  36  37  38  39  40  41  42  43  45  46
 [40]  47  50  51  53  54  55  56  58  59  60  61  62  63
 [53]  64  65  66  67  68  70  71  73  75  77  78  79  81
 [66]  82  83  85  86  88  91  92  93  94  95  96  99 100
 [79] 101 102 103 104 105 106 108 110 111 112 113 114 115
 [92] 116 117 118 119 121 122 123 124 126 128 129 130 131
[105] 132 133 134 135 136 137 138 139 140 141 142 144 145
[118] 147 149 150 151 152 153 154 155 156 157 158 159 160
[131] 161 163 164 165 166 167 169 170 171 172 173 174 175
[144] 176 177 178 180 181 182 183 184 185 186 187 188 189
[157] 190 193 195 196 198 199 200 201 202 203 204 205 206
[170] 207 208 209 210 212 214 215 216 217 218 220 221 222
[183] 223 224 225 226 227 228 229 231 232 234 236 237 238
[196] 239 240 241 242 243 244 245 246 248 250 253 255 256
[209] 258 260 261 262 263 264 265 266 267 268 270 272 273
[222] 275 276 277 278 279 280 281 282 284 285 286 287 288
[235] 289 291 293 294 295 296 297 299 300 301 302 304 305
[248] 306 307 308 309 311 312 313 314 315 316 317 318 320
[261] 321 322 323 324 325 326 327 328 329 330 331 332 333
[274] 334 335 336 337 338 340 341 342 343 344 345 346 347
[287] 348 349 350 351 352 353 355 356 357 358 359 360 361
[300] 362 363 364 365 368 369 370 371 374 375 376 377 378
[313] 379 380 381 382 383 384 385 386 387 388 389 390 391
[326] 392 393 394 395 399 400 401 402 404 405 407 409 411
[339] 412 415 416 417 418 419 420 422 423 425 426 427 428
[352] 431 432 433 434 435 437 438 439 440 442 443 447 451
[365] 452 455 456 457 459 460 461 463 464 467 468 469 472
[378] 474 475 476 477 478 479 481 482 484 485 486 487 488
[391] 489 491 492 493 494 495 496 497 498 499
> testfolds[1]
$Fold1
  [1]   4   6   7   9  13  21  44  48  49  52  57  69  72
 [14]  74  76  80  84  87  89  90  97  98 107 109 120 125
 [27] 127 143 146 148 162 168 179 191 192 194 197 211 213
 [40] 219 230 233 235 247 249 251 252 254 257 259 269 271
 [53] 274 283 290 292 298 303 310 319 339 354 366 367 372
 [66] 373 396 397 398 403 406 408 410 413 414 421 424 429
 [79] 430 436 441 444 445 446 448 449 450 453 454 458 462
 [92] 465 466 470 471 473 480 483 490 500

> folds = createFolds(x1,5)
> for(k in 1:5){
+     removed = folds[k]
+     svm.fit = svm(y~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,xdf[removed,])
+     error = sum(ifelse(svm.prob==y[removed],0,1))/100
+ }
Error in -removed : invalid argument to unary operator
> folds[1]
$Fold1
  [1]   2   5   8  12  16  28  29  32  36  37  43  44  46
 [14]  50  56  61  63  69  70  75  76  86  87  90  92  98
 [27] 103 107 109 110 116 122 123 131 151 152 153 159 162
 [40] 163 167 170 174 178 181 190 191 193 202 203 205 208
 [53] 214 217 219 222 232 233 234 253 257 258 264 266 267
 [66] 280 290 294 301 313 326 330 331 335 339 354 355 357
 [79] 359 363 365 366 368 371 376 380 391 394 409 418 434
 [92] 438 442 459 460 478 481 489 493 494

> xdf[-removed,]
Error in -removed : invalid argument to unary operator
> xdf[-folds[1],]
Error in -folds[1] : invalid argument to unary operator
> xdf[-folds[1]]
Error in -folds[1] : invalid argument to unary operator
> xdf[folds[1]]
Error in `[.default`(xdf, folds[1]) : invalid subscript type 'list'
> xdf[as.numeric(folds[1])]
Error in `[.data.frame`(xdf, as.numeric(folds[1])) : 
  (list) object cannot be coerced to type 'double'
> sample(200,2)
[1] 198 117
> ddd = sample(200,2)
> xdf[as.integer(folds[1])]
Error in `[.data.frame`(xdf, as.integer(folds[1])) : 
  (list) object cannot be coerced to type 'integer'
> as.intereger(folds[1])
Error in as.intereger(folds[1]) : could not find function "as.intereger"
> as.integer(folds[1])
Error: (list) object cannot be coerced to type 'integer'
> as.numeric(unlist(folds[1]))
  [1]   2   5   8  12  16  28  29  32  36  37  43  44  46
 [14]  50  56  61  63  69  70  75  76  86  87  90  92  98
 [27] 103 107 109 110 116 122 123 131 151 152 153 159 162
 [40] 163 167 170 174 178 181 190 191 193 202 203 205 208
 [53] 214 217 219 222 232 233 234 253 257 258 264 266 267
 [66] 280 290 294 301 313 326 330 331 335 339 354 355 357
 [79] 359 363 365 366 368 371 376 380 391 394 409 418 434
 [92] 438 442 459 460 478 481 489 493 494
> xdf[unlist(folds[1]),]
    y           x1           x2
2   1 -0.039821265 -0.395706996
5   0 -0.484300241 -0.090992216
8   1 -0.264461913 -0.311598535
12  1  0.342559406 -0.105541257
16  0  0.160064494  0.366045067
28  1  0.066665736 -0.223983621
29  0 -0.367771122 -0.137638745
32  0 -0.288343362  0.489588621
36  0  0.074419082  0.223776752
37  0 -0.230989026  0.304538233
43  1  0.088924153 -0.084713329
44  0  0.355776246  0.485645169
46  0 -0.017021387  0.061434917
50  1  0.315533990 -0.233630792
56  1  0.323823029 -0.008143849
61  0  0.260837264  0.324285479
63  0  0.412628801  0.456202116
69  0 -0.133992770  0.105076495
70  1  0.186994071 -0.278479760
75  0  0.232169085  0.235650823
76  1 -0.061863864 -0.360389696
86  1  0.141786638 -0.447572930
87  1  0.101090547 -0.329839510
90  0 -0.198667221  0.232878983
92  0  0.467904175  0.457710524
98  1 -0.391245153 -0.386949960
103 0  0.035857872  0.281915700
107 0  0.496422897  0.405003709
109 1  0.155455282 -0.474421893
110 1  0.014034537 -0.345545398
116 0  0.486130726  0.439940616
122 1 -0.007077476 -0.151363859
123 1  0.238244612 -0.064563001
131 0 -0.147558670  0.123988073
151 1 -0.059651955 -0.467611580
152 0 -0.408502292  0.336676417
153 1 -0.134528378 -0.241344785
159 1  0.285424602 -0.357940820
162 0 -0.088661532  0.021752474
163 0 -0.316032706  0.314411741
167 1  0.424234301 -0.478007222
170 0 -0.345117470 -0.024533085
174 1  0.003018740 -0.266332365
178 0 -0.007389633  0.272387899
181 1  0.416541562 -0.114689425
190 0 -0.180833028  0.300261724
191 1 -0.445016443 -0.243054271
193 1  0.066911929 -0.406431218
202 0 -0.215184460  0.179677677
203 0 -0.246709316  0.450924571
205 0 -0.433594510 -0.106782555
208 0  0.013006821  0.163254071
214 0 -0.225266470  0.093375277
217 0 -0.362769425  0.376125638
219 1  0.388351332 -0.430694188
222 1  0.414449882 -0.118257221
232 0 -0.132793600  0.391584668
233 1  0.435213899 -0.009479322
234 0 -0.017926824  0.399790396
253 0 -0.177445801  0.345129881
257 0  0.051814709  0.421278582
258 1  0.125628977  0.023389946
264 0 -0.398501381  0.037663502
266 1 -0.240152922 -0.154607306
267 0 -0.082884835  0.459440651
280 0 -0.147232567  0.314432969
290 0 -0.267681908 -0.022375949
294 1  0.055399674 -0.321248284
301 0  0.474835206  0.292850044
313 1  0.157265123 -0.315167864
326 0 -0.401337562 -0.069089023
330 0 -0.394757155  0.161336964
331 0 -0.251694867  0.203760031
335 1  0.048282872 -0.409067587
339 0 -0.307434262  0.241741939
354 0  0.157449533  0.479590325
355 0 -0.310997013 -0.036594921
357 0 -0.403711127 -0.185243523
359 1  0.321055961 -0.374567046
363 1  0.187350965  0.080761198
365 1  0.418641011 -0.174641900
366 1  0.058743946 -0.405590919
368 1  0.030842151 -0.276713180
371 0 -0.369459561  0.282438014
376 0 -0.202237033  0.153123466
380 1 -0.267802584 -0.181345513
391 1  0.477621913 -0.464212604
394 1 -0.170679248 -0.295225768
409 0 -0.268159761  0.267465901
418 0  0.420008110  0.492506007
434 0  0.334195255  0.292787435
438 0  0.205500675  0.202342115
442 0 -0.275437627  0.471761883
459 1  0.409501710 -0.033972207
460 1  0.099068362 -0.406311998
478 0 -0.185994303  0.157425460
481 0 -0.108673870  0.219077961
489 0 -0.096764165  0.199073123
493 1  0.227787792 -0.227761687
494 1  0.485756573 -0.387875930
> xdf[-unlist(folds[1]),]
    y            x1            x2
1   0 -0.0814355009  0.1280802540
3   0 -0.2716106409  0.1764477571
4   1  0.4201503275  0.2032831332
6   0  0.0714519317  0.4523481724
7   1  0.4690748958 -0.3764157228
9   0 -0.1028039926  0.2269113320
10  0 -0.1209069428  0.1602083351
11  1  0.2990930974 -0.3403947579
13  0 -0.4344612511  0.0925926808
14  1  0.0931437663 -0.1977166671
15  1  0.4209675624  0.0113802343
17  0 -0.3049957359  0.1908167328
18  1  0.4897362534 -0.2575643968
19  1 -0.1072578304 -0.2699079912
20  1  0.3509937783  0.0096845205
21  1 -0.1958691874 -0.4144587805
22  0 -0.3141070106  0.1593807330
23  1 -0.1925408570 -0.3720800022
24  0 -0.2236965175  0.4586900293
25  1 -0.3769454060 -0.3646824150
26  0  0.4196943163  0.3794658871
27  1  0.2702335988 -0.4700472443
30  0  0.3898966776  0.2169881505
31  0 -0.1388473343  0.3759261095
33  1  0.2291979515  0.0551322075
34  0  0.3252181041  0.2658739274
35  0 -0.0478106693  0.0127671920
38  1  0.4722604936  0.1503119299
39  1  0.1583402483 -0.3164779216
40  0  0.4223000775  0.3919021667
41  0 -0.0326950883  0.2780322407
42  0 -0.0594159195 -0.0133567159
45  1 -0.0936672348 -0.3708679448
47  0 -0.4511658256 -0.1471316158
48  1 -0.2003353029 -0.4084067466
49  1  0.1247430383  0.0074315241
51  1  0.1541820567 -0.1913576350
52  0  0.2697919789  0.2879910683
53  0 -0.1199640236  0.3754424260
54  1 -0.3282396609 -0.4077396237
55  1  0.3145658185 -0.1251355580
57  0 -0.1214290641 -0.0192413758
58  1  0.2587696624 -0.0200218810
59  1  0.2795240197 -0.1971867508
60  0 -0.4896958161  0.1731558880
62  0 -0.3719167393  0.2644488220
64  1 -0.3516557075 -0.3400739215
65  0 -0.0778561411  0.2285732394
66  0  0.0507461533  0.0833556966
67  1  0.4121574641 -0.3084744464
68  0  0.0850091306  0.2784172865
71  0 -0.2140469702  0.1391384236
72  1 -0.3486928721 -0.4575678764
73  0 -0.3312691988  0.0509251321
74  0  0.1140209520  0.3974830776
77  1 -0.1589622009 -0.4696634603
78  0 -0.1145298460  0.4211694014
79  0 -0.2495839440 -0.0326420413
80  1  0.2379051819 -0.3776526777
81  0  0.2308745713  0.2653836231
82  1  0.0027636357 -0.3420137092
83  0 -0.4918336433  0.3669542808
84  0 -0.1849086988 -0.0754459589
85  1 -0.3801150972 -0.3574280939
88  1  0.0553744182 -0.4832480589
89  0 -0.0356475145  0.1294366270
91  1  0.2048158313  0.0883708601
93  0 -0.1102815822  0.0863753937
94  1  0.0209199958 -0.2499655546
95  1  0.0881106241 -0.0112081191
96  1 -0.0853114955 -0.2634445715
97  0  0.3857608149  0.3605264097
99  1  0.4192117394 -0.0070486364
100 0  0.4167018863  0.2274100943
101 0 -0.1896663480  0.1083195792
102 1 -0.0817049399 -0.4817906844
104 0 -0.2320116502  0.1551014709
105 0  0.2911178691  0.2246897598
106 0  0.1838243646  0.4298059216
108 1 -0.0382457233 -0.4477100440
111 1  0.4956695307 -0.2950075252
112 0 -0.1480244312 -0.0489658127
113 0 -0.4444214290 -0.0610876759
114 0  0.0920472878  0.4800659262
115 0  0.1253408694  0.2273289326
117 0  0.1797575552  0.3801864516
118 1  0.2012268621 -0.1008165991
119 1 -0.3042447646 -0.4022942709
120 1 -0.0646415590 -0.1107174801
121 0 -0.0689403501  0.3339493556
124 1  0.2058243407  0.0056030364
125 0 -0.4164709218  0.1867899653
126 1 -0.3239046279 -0.3250924780
127 1 -0.1121094329 -0.2295559181
128 1  0.0494450198 -0.0113381329
129 1  0.0241861846 -0.2480148110
130 0  0.2228415450  0.1754142921
132 1 -0.2424211313 -0.3502217089
133 1  0.0961070112  0.0323824103
134 1  0.4029765045 -0.0574399731
135 1  0.3084348741 -0.0772789090
136 1  0.1813217846 -0.3915543172
137 1  0.2883248860 -0.0045507697
138 0 -0.4704906268 -0.1062909204
139 1 -0.3354517145 -0.4480655743
140 1  0.4956585050  0.0326095363
141 0 -0.3968566875 -0.1312509407
142 1 -0.1201016065 -0.3863761819
143 1  0.2159611084 -0.3525325493
144 1  0.4911392385  0.1394523834
145 0  0.1910232881  0.1841638647
146 0 -0.2379458270  0.2363542770
147 1  0.4061589737 -0.3954538798
148 0 -0.3150782534  0.2576679448
149 1 -0.0010152429 -0.3291597220
150 0 -0.1224934838  0.4757482449
154 0 -0.1842161932  0.2568985431
155 0  0.3014432001  0.4155009547
156 1 -0.1531142697 -0.3662793243
157 1  0.3379046845 -0.3223202750
158 0  0.4109380024  0.4265642816
160 0  0.1345011087  0.3609346335
161 1 -0.3271403499 -0.2764606229
164 1  0.1917015100 -0.4228052602
165 1  0.1803791386 -0.1596267140
166 1  0.0603687048 -0.2729979281
168 1 -0.3586644002 -0.2975370158
169 0 -0.3972420639  0.4071046114
171 1  0.2824886213 -0.3695643635
172 1 -0.1107815958 -0.2568851754
173 0 -0.3505625376  0.1774756925
175 0  0.1618718151  0.1781478990
176 0  0.1354924170  0.3408031941
177 0 -0.2828487679  0.4807217633
179 1 -0.0830463103 -0.4039437587
180 0 -0.4790232799  0.2115967115
182 1  0.1891385787 -0.4388765157
183 1 -0.0207784877 -0.4644931410
184 1  0.1585662926 -0.3837059538
185 1  0.4513609188 -0.1957418036
186 0 -0.1843969850 -0.0492739119
187 1  0.2854552602  0.0873696594
188 0  0.1353730937  0.3308602853
189 0 -0.2237259359  0.4406512091
192 1  0.0911874741 -0.0721195654
194 1 -0.3389286113 -0.4965974526
195 1  0.0664291764 -0.0857677998
196 1 -0.3277117743 -0.1873604129
197 0  0.0140143270  0.4069029791
198 0 -0.1081160130  0.0076369338
199 1 -0.1300407122 -0.1476657940
200 0 -0.4294049924  0.0904163949
201 1 -0.0413874874 -0.3943796142
204 1  0.4080909160  0.1332927160
206 1 -0.0005306483 -0.4121673142
207 1  0.4449577199 -0.3272533789
209 0  0.0971927629  0.3519928153
210 0 -0.2978532689  0.4848323977
211 1 -0.3473361058 -0.2144701062
212 0 -0.3013708235  0.3145401764
213 0  0.1744186196  0.3652072968
215 0  0.2106255707  0.1078725257
216 0 -0.1552890663  0.1488940874
218 0 -0.4145660843  0.4465009943
220 1 -0.4222796117 -0.3528716960
221 0  0.4381884320  0.3483040405
223 1  0.0069223207 -0.4513830815
224 0 -0.2828794888  0.2406958195
225 0  0.0365150345  0.2703036920
226 1 -0.2768052686 -0.2420234338
227 0 -0.0800908525  0.0742311333
228 0  0.0416918923  0.2646512319
229 0 -0.2359716555  0.1195276009
230 1  0.4428051892  0.1473848191
231 1  0.0097011989 -0.3864961825
235 1  0.2297394027  0.0541793092
236 0 -0.3162324687 -0.1548891414
237 1  0.2803329488  0.0866013141
238 1  0.0516732377 -0.0302889163
239 0 -0.2389905860  0.3907738703
240 1 -0.1000206009 -0.1694108977
241 0 -0.2870081351  0.0820850080
242 0 -0.1549034151  0.2147396985
243 0 -0.1144166188  0.1011924185
244 1 -0.1248290411 -0.1139390511
245 0  0.4626939583  0.3008030460
246 1  0.2254782489 -0.2864886411
247 1  0.1760862991 -0.0018434308
248 0 -0.0191959965  0.4887515414
249 0 -0.0951372965  0.0915542238
250 1 -0.3198858507 -0.2951612400
251 1  0.3718807749 -0.2437064685
252 1 -0.1394145798 -0.4933790441
254 0  0.2740220779  0.4644959804
255 0  0.2153361561  0.2397502519
256 0  0.4914234302  0.2481610500
259 1 -0.0012436858 -0.1838156765
260 0  0.2177686326  0.4071954468
261 0 -0.4616540580  0.0219196558
262 0  0.3604858350  0.3300843132
263 1  0.4873526564  0.0419614541
265 1 -0.1047075198 -0.3561369479
268 0 -0.3150535065 -0.0155838989
269 0 -0.4312051369  0.2083747583
270 1  0.2399931494 -0.3497718901
271 1  0.0128223347 -0.2791895431
272 0  0.1597691486  0.3544081743
273 1  0.3429669235 -0.1430529591
274 1 -0.4998109052 -0.3566294413
275 0  0.4206815481  0.4630861259
276 1 -0.4625937883 -0.4428933684
277 0 -0.4073136507  0.4259328672
278 1  0.4287681652  0.1158503408
279 1  0.3214065582 -0.3327786957
281 0 -0.4621559069 -0.1535848246
282 1 -0.3760890469 -0.4928559628
283 0 -0.4468906263  0.1806650362
284 1 -0.2734037891 -0.4190769363
285 0 -0.1845129335 -0.0009805546
286 1  0.3063687291  0.1023134307
287 1 -0.1274347596 -0.0823083124
288 1  0.0675189183 -0.0631403732
289 0 -0.4225342115  0.3419690013
291 0  0.0493816873  0.3963268567
292 1 -0.4042092357 -0.4001433060
293 0  0.4225331582  0.3214088459
295 0 -0.0010718512  0.1547721701
296 1  0.4101211410 -0.4719204786
297 0 -0.1375218099 -0.0352434439
298 1  0.0784339858 -0.2122142473
299 0  0.0230670467  0.2321828527
300 0 -0.0536407295  0.2711785312
302 0 -0.0132477426  0.4725153726
303 1  0.2380345562 -0.3081147647
304 1 -0.4029797581 -0.2857586395
305 1  0.0582753229 -0.1686146322
306 1  0.3828928869 -0.3204009202
307 0 -0.4191483066 -0.0390139811
308 1  0.3753769691 -0.0119068646
309 1 -0.1841663120 -0.2444942417
310 0 -0.2454160990  0.3248999342
311 1  0.2410357504 -0.2517738270
312 0 -0.4931165329 -0.1481407757
314 1 -0.4940190630 -0.3956007925
315 0  0.2859334887  0.4234777680
316 1  0.0946178394 -0.1296048963
317 1 -0.0759409724 -0.3390722140
318 0 -0.3991783101 -0.0523453820
319 1 -0.0054091609 -0.1245625056
320 0  0.2727058285  0.4547082626
321 0 -0.1789447109  0.0567233514
322 0 -0.0176475297  0.3069633367
323 0  0.1020754157  0.4206111352
324 1  0.1099537613 -0.0463926278
325 1  0.1254942338 -0.3748679489
327 0 -0.3273512952  0.2036224280
328 1 -0.2748798979 -0.2307959853
329 0  0.1440432756  0.2361880415
332 1 -0.0070210509 -0.3428939644
333 1  0.4168010752 -0.4010254878
334 1  0.0516493549 -0.4543001319
336 0 -0.1874482115  0.1798297882
337 1 -0.1996404238 -0.4967315034
338 0 -0.3282244878  0.0511463117
340 1  0.3327954630 -0.3504028344
341 1 -0.0311544980 -0.2256744148
342 0 -0.4279995763 -0.0632684180
343 0 -0.3113657488  0.1317442439
344 1 -0.2091068078 -0.1848304591
345 1  0.1616929835 -0.3034177779
346 0 -0.2966661984  0.1633088416
347 1 -0.2235081526 -0.2237043213
348 1  0.4138844111 -0.4219511161
349 1 -0.2411559115 -0.3855347084
350 1 -0.4382005308 -0.3136963549
351 0  0.0867920853  0.2916067254
352 1  0.3818440980 -0.3277248682
353 1 -0.1706432642 -0.2604884265
356 1  0.0893897000 -0.0823034314
358 0 -0.4774811957 -0.1978734997
360 1  0.1734025520 -0.1215596281
361 0 -0.4496006228  0.4074023524
362 0 -0.0058942460  0.1241613647
364 1 -0.3080111283 -0.1898507481
367 1  0.4453880519 -0.0476743872
369 1  0.3002163284 -0.2864761401
370 1 -0.2591281026 -0.4633874013
372 1  0.3770917475 -0.4862551580
373 1  0.3128643204 -0.2524057759
374 0  0.4581361290  0.4690208600
375 1  0.3916921723 -0.3989429653
377 1  0.2267580617 -0.0250026726
378 1 -0.1027536509 -0.4219119498
379 1 -0.0665085360 -0.3818742007
381 1 -0.1564856735 -0.1120097756
382 0  0.0468796750  0.4929380936
383 1  0.0139405422 -0.4145090394
384 0 -0.3878911890  0.0481743091
385 1  0.4801364106 -0.4851887997
386 0 -0.4268484269  0.4554762840
387 0 -0.2352153100  0.4362951473
388 0 -0.4599556476  0.0928661288
389 1  0.2249493550 -0.0369563315
390 0 -0.2643173225  0.0014834367
392 0 -0.4261393650 -0.1198556495
393 0 -0.1803806648  0.4883466135
395 1  0.3986599848 -0.4916900482
396 0 -0.4798713592 -0.0759242757
397 0 -0.2946289161  0.3179437891
398 1  0.4908611616 -0.2895267105
399 0  0.3058661309  0.1773723974
400 1 -0.1463274534 -0.1711355699
401 0  0.4761979338  0.3252687154
402 0 -0.2512866242  0.0690205183
403 1 -0.0689758093 -0.1998780202
404 0 -0.3308508107  0.0710364094
405 0 -0.4091838277  0.3586098307
406 0 -0.2783185395  0.1327623781
407 0 -0.4056381509  0.2772878855
408 0 -0.4545023413  0.3819835261
410 1  0.1449277555 -0.1388061787
411 0 -0.1171559498  0.4426788623
412 0 -0.1896018218  0.0629807776
413 1  0.1988405257  0.0600524540
414 0 -0.3725068884 -0.0406932032
415 0  0.1503239458  0.1085158493
416 1  0.3867938486  0.1036760400
417 1 -0.2133591960 -0.3072359979
419 1  0.4212520558  0.0927196348
420 0 -0.0146109476  0.4956227422
421 0 -0.0211025388  0.1773847542
422 1  0.3454515627 -0.0478911158
423 1 -0.1734524309 -0.3180856279
424 0  0.1750247618  0.3896222918
425 1  0.2564476500 -0.0424699499
426 1  0.0079163290 -0.2950970819
427 1  0.3390370272 -0.1750943537
428 1 -0.3253816268 -0.2874959635
429 1  0.1146173258 -0.0186759608
430 0  0.0854938102  0.1419093660
431 0 -0.3338935371  0.1352992007
432 0 -0.0773042596 -0.0093355265
433 0  0.1969293978  0.2271064236
435 0 -0.0779535742  0.3667728803
436 0  0.0680442420  0.0537019162
437 0 -0.3392385603  0.0171256249
439 0  0.3901247827  0.2327219429
440 0  0.1725385122  0.4982088457
441 1  0.3072832781  0.0323841921
443 1 -0.3411101226 -0.3686592828
444 1  0.3918077426 -0.0774572801
445 0  0.2452975714  0.3423763490
446 1 -0.2019231215 -0.3291375078
447 0 -0.1077781357  0.1410734034
448 0 -0.3659071180 -0.1002648075
449 1 -0.2102096404 -0.2129458610
450 0 -0.4520371351  0.3650332713
451 1  0.3340824312 -0.4025225835
452 0  0.4001501570  0.3009881333
453 1  0.3870051671 -0.2240467765
454 0  0.4952546007  0.4451587128
455 0 -0.1372227287  0.1632607447
456 0  0.4951788848  0.3968252931
457 0 -0.3983124387  0.3207734555
458 1 -0.0941294176 -0.2034429319
461 1  0.4226107467 -0.2590933146
462 0 -0.3777519080  0.1884247877
463 0  0.3364357455  0.2189333418
464 0 -0.4022515698  0.4410786417
465 1  0.3260858855 -0.3675587401
466 1  0.1920733512 -0.2994320097
467 0 -0.0794775095  0.0835860104
468 1 -0.0002956649 -0.1206501264
469 0  0.3519512725  0.3518841832
470 1  0.1414716146  0.0202369716
471 0 -0.1306340406  0.4335646196
472 1  0.0027868715 -0.4652125363
473 0 -0.3866739771  0.2665300597
474 0 -0.1905363456  0.2260450283
475 1  0.4214480293 -0.2718900021
476 0 -0.3311791874 -0.1250528803
477 0  0.0193757801  0.1222060155
479 1  0.1005023492 -0.1420844488
480 0 -0.0548399598  0.4203656295
482 1  0.3904233880  0.1421876769
483 1  0.3957968508 -0.1566584802
484 0 -0.3670243034  0.3870940187
485 0  0.1124960785  0.4431080045
486 0 -0.2337815613  0.3910768598
487 1  0.2177699667 -0.1776085894
488 0 -0.4259024882  0.3038999073
490 0 -0.2265826929  0.4690410506
491 0  0.2275900708  0.4216944207
492 1  0.1404648896 -0.3544606627
495 1  0.3115358723 -0.4376271563
496 1  0.4632994928  0.2241974743
497 1  0.1151229450 -0.1818390212
498 1 -0.1465947411 -0.4165439692
499 0  0.0360343200  0.3692111634
500 0 -0.2305019142  0.1789461535
> for(k in 1:5){
+     removed = folds[k]
+     svm.fit = svm(y~.,data=xdf[-unlist(removed),],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,xdf[removed,])
+     error = sum(ifelse(svm.prob==y[removed],0,1))/100
+ }
Error in xj[i] : invalid subscript type 'list'
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(y~.,data=xdf[-emoved,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,xdf[removed,])
+     error = sum(ifelse(svm.prob==y[removed],0,1))/100
+ }
Error in `[.data.frame`(xdf, -emoved, ) : object 'emoved' not found
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(y~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,xdf[removed,])
+     error = sum(ifelse(svm.prob==y[removed],0,1))/100
+ }
> out = ifelse(svm.prob==y[removed],0,1)
> out = ifelse(svm.pred==y[removed],0,1)
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(y~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,xdf[removed,])
+     error = sum(ifelse(svm.pred==y[removed],0,1))/100
+ }
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(y~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=xdf[removed,])
+     error = sum(ifelse(svm.pred==y[removed],0,1))/100
+ }
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=xdf[removed,])
+     error = sum(ifelse(svm.pred==y[removed],0,1))/100
+ }
> kerror = double(5)
> for(k in 1:5){
+     removed = unlist(folds[k])
+     svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=xdf[removed,])
+     kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+ }
> costerrors = double(21)
> for(c in seq(from=0,to=20,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+     }
+     costerrors(c) = mean(kerror)
+ }
Error in costerrors(c) <- mean(kerror) : 
  could not find function "costerrors<-"
> for(c in seq(from=0,to=20,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
> costerrors
 [1] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
[10] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
[19] 0.004 0.004 0.000
> for(c in seq(from=0,to=100,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=5,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
> costerrors
  [1] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
  [9] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [17] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [25] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [33] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [41] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [49] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [57] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [65] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [73] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [81] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [89] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [97] 0.004 0.004 0.004 0.004
> for(c in seq(from=0,to=100,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=c,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
Error in svm.default(x, y, scale = scale, ..., na.action = na.action) : 
  C <= 0!
> for(c in seq(from=1,to=100,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=c,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==y[removed],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
> costerrors
  [1] 0.004 0.004 0.004 0.004 0.004 0.004 0.002 0.004
  [9] 0.004 0.004 0.004 0.004 0.004 0.004 0.002 0.002
 [17] 0.002 0.002 0.002 0.002 0.002 0.004 0.004 0.004
 [25] 0.002 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [33] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [41] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [49] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [57] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.002
 [65] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [73] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [81] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [89] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [97] 0.002 0.002 0.002 0.002
> 
> x3 = runif(500) -.5
> error = rnorm(500)
> x3 = runif(500) -.5
> x4 = runif(500) -.5
> y = 1*(.5*x3 - x4 > 0)
> newdf = data.frame(as.factor(y),x3,x4)
> svm.fit = svm(factor(y)~.,data=xdf,kernel="linear",cost=5,scaled=FALSE)
> svm.predict = predict(svm.fit,newdf)
> error = sum(ifelse(svm.predict==newdf$as.factor.y.,0,1))/500
> error = sum(ifelse(svm.predict==as.factor(newdf$y),0,1))/500
Error in Ops.factor(svm.predict, as.factor(newdf$y)) : 
  level sets of factors are different
> error = sum(ifelse(svm.predict==newdf$y.,0,1))/500
Warning message:
In is.na(e2) : is.na() applied to non-(list or vector) of type 'NULL'
> mean(costerrors)
[1] 0.00308
> error = sum(ifelse(svm.predict==newdf$y.,0,1))
Warning message:
In is.na(e2) : is.na() applied to non-(list or vector) of type 'NULL'
> error = sum(ifelse(svm.predict==newdf$as.factor.y.,0,1))
> error = sum(ifelse(svm.predict==newdf$as.factor.y.,0,1))/500
> error
[1] 0.508
> for(c in seq(from=1,to=100,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=c,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==xdf[removed,3],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
> costerrors
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [27] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [53] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [79] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> View(xdf)
> for(c in seq(from=1,to=100,by=1)){
+     kerror = double(5)
+     for(k in 1:5){
+         removed = unlist(folds[k])
+         svm.fit = svm(factor(y)~.,data=xdf[-removed,],kernel="linear",cost=c,scaled=FALSE)
+         svm.pred = predict(svm.fit,newdata=xdf[removed,])
+         kerror[k] = sum(ifelse(svm.pred==xdf[removed,1],0,1))/100
+     }
+     costerrors[c] = mean(kerror)
+ }
> costerrors
  [1] 0.004 0.004 0.004 0.004 0.004 0.004 0.002 0.004
  [9] 0.004 0.004 0.004 0.004 0.004 0.004 0.002 0.002
 [17] 0.002 0.002 0.002 0.002 0.002 0.004 0.004 0.004
 [25] 0.002 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [33] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [41] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [49] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.004
 [57] 0.004 0.004 0.004 0.004 0.004 0.004 0.004 0.002
 [65] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [73] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [81] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [89] 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002
 [97] 0.002 0.002 0.002 0.002
> mean(costerrors)
[1] 0.00308
> morecosterrors = double(100)
> for(c in seq(from=1,to=100,by=1)){
+     svm.fit = svm(factor(y)~.,data=xdf,kernel="linear",cost=c,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=newdf)
+     morecosterrors[c] = sum(ifelse(svm.pred==newdf$as.factor.y,0,1))/500
+ 
+ }
> morecosterrors
  [1] 0.514 0.510 0.508 0.508 0.508 0.508 0.508 0.508
  [9] 0.508 0.508 0.508 0.508 0.508 0.508 0.508 0.512
 [17] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [25] 0.512 0.512 0.512 0.512 0.514 0.514 0.514 0.514
 [33] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [41] 0.514 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [49] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [57] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [65] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [73] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [81] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [89] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [97] 0.514 0.514 0.514 0.514
> mean(morecosterrors)
[1] 0.51246
> y = 1*(.5*x1 - x2 > 0)
> morecosterrors = double(100)
> for(c in seq(from=1,to=100,by=1)){
+     svm.fit = svm(factor(y)~.,data=xdf,kernel="linear",cost=c,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=newdf)
+     morecosterrors[c] = sum(ifelse(svm.pred==newdf$as.factor.y,0,1))/500
+ 
+ }
> morecosterrors
  [1] 0.514 0.510 0.508 0.508 0.508 0.508 0.508 0.508
  [9] 0.508 0.508 0.508 0.508 0.508 0.508 0.508 0.512
 [17] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [25] 0.512 0.512 0.512 0.512 0.514 0.514 0.514 0.514
 [33] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [41] 0.514 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [49] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [57] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [65] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [73] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [81] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [89] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [97] 0.514 0.514 0.514 0.514
> View(newdf)
> morecosterrors = double(100)
> for(c in seq(from=1,to=100,by=1)){
+     svm.fit = svm(factor(y)~.,data=xdf,kernel="linear",cost=c,scaled=FALSE)
+     svm.pred = predict(svm.fit,newdata=newdf[,-1])
+     morecosterrors[c] = sum(ifelse(svm.pred==newdf$as.factor.y,0,1))/500
+ }
> morecosterrors
  [1] 0.514 0.510 0.508 0.508 0.508 0.508 0.508 0.508
  [9] 0.508 0.508 0.508 0.508 0.508 0.508 0.508 0.512
 [17] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [25] 0.512 0.512 0.512 0.512 0.514 0.514 0.514 0.514
 [33] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [41] 0.514 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [49] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [57] 0.512 0.512 0.512 0.512 0.512 0.512 0.512 0.512
 [65] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [73] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [81] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [89] 0.514 0.514 0.514 0.514 0.514 0.514 0.514 0.514
 [97] 0.514 0.514 0.514 0.514

