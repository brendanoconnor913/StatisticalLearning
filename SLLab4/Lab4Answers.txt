2.
b/c)
validationerror = function(data) {
    train = sample(dim(data)[1], dim(data)[1]/2)
    glm.fit = glm(default~income+balance, data, family="binomial", subset=train)
    fit.prob = predict(glm.fit, type="response", newdata=data[-train,])
    glm.pred = ifelse(fit.prob > .5, "Up", "Down")
    t = table(glm.pred, data[-train,]$default)
    missclassified = t[1,2] + t[2,1]
    (missclassified / sum(t))
}

> set.seed(1)
> validationerror(Default)
[1] 0.0286
> validationerror(Default)
[1] 0.0236
> validationerror(Default)
[1] 0.028
> validationerror(Default)
[1] 0.0268

It looks like there isn't a ton of variance just off of the few samples
that we calculated.

d)
> validationerror = function(data) {
+     train = sample(dim(data)[1], dim(data)[1]/2)
+     glm.fit = glm(default~income+balance+student, data, family="binomial", subset=train)
+     fit.prob = predict(glm.fit, type="response", newdata=data[-train,])
+     glm.pred = ifelse(fit.prob > .5, "Up", "Down")
+     t = table(glm.pred, data[-train,]$default)
+     missclassified = t[1,2] + t[2,1]
+     (missclassified / sum(t))
+ }
> validationerror(newDefault)
[1] 0.0282
> validationerror(newDefault)
[1] 0.0282
> validationerror(newDefault)
[1] 0.0282
> validationerror(newDefault)
[1] 0.0278
> validationerror(newDefault)
[1] 0.027

It definitely had an effect on the validation error. It did increase it 
but it also seems to decrease the variance of the results.

3.
d)
> summary(glm.fit)$coeff
                 Estimate   Std. Error    z value      Pr(>|z|)
(Intercept) -1.154047e+01 4.347564e-01 -26.544680 2.958355e-155
balance      5.647103e-03 2.273731e-04  24.836280 3.638120e-136
income       2.080898e-05 4.985167e-06   4.174178  2.990638e-05

Bootstrap Statistics :
         original        bias     std. error
t1* -1.154047e+01 -7.025924e-03 4.256407e-01
t2*  5.647103e-03  1.762463e-06 2.271911e-04
t3*  2.080898e-05  4.635898e-08 4.599874e-06

The estimates provdied are very similar which seems to indicate the 
assumptions for the glm calculations hold on the data.

4.
a)
> mean(medv)
[1] 22.53281

b)
> mstderr = sd(medv)/sqrt(dim(Boston)[1])
> mstderr
[1] 0.4088611

c)
> boo.fn = function(data, index)
+     return(mean(data[index,]$medv))
> boot(Boston,boo.fn,R=10000)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Boston, statistic = boo.fn, R = 10000)


Bootstrap Statistics :
    original       bias    std. error
t1* 22.53281 -0.005211166   0.4075966

The errors are pretty similar with .4088 and .4075.

d)
> t.test(medv)

    One Sample t-test

data:  medv
t = 55.111, df = 505, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 21.72953 23.33608
sample estimates:
mean of x 
 22.53281 

> m-2*mstderr
[1] 21.71508
> m+2*mstderr
[1] 23.35053

We see that the confidence intervals are (much as expected) very similar
in both calculations.

e)
> med = median(medv)
> med
[1] 21.2

f)
> boot(Boston,boot.fn,R=10000)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Boston, statistic = boot.fn, R = 10000)


Bootstrap Statistics :
    original   bias    std. error
t1*     21.2 -0.02035   0.3811409

We see from the bootstrap that the standard error is nearly .4 which 
isn't terrible but far from narrow.

g)
> quantile(medv, c(.1))
  10% 
12.75 
h)
> boot.fn = function(data, index)
+     return(quantile(data[index,]$medv, c(.1)))
> boot(Boston,boot.fn,R=10000)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Boston, statistic = boot.fn, R = 10000)


Bootstrap Statistics :
    original    bias    std. error
t1*    12.75 -0.001225   0.5070083

The standard error seems fairly similar to the error for all of the 
other values which were around .4 or so. This seems to hold for the 
statistics we have calculated.
